{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hydci1aUCXaX"
      },
      "outputs": [],
      "source": [
        "!pip install google-auth\n",
        "from google.colab import auth\n",
        "auth.authenticate_user()\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "!ls /content/drive/MyDrive\n",
        "import os\n",
        "\n",
        "path = '/content/drive/My Drive'\n",
        "\n",
        "if os.path.isdir(path):\n",
        "    contents = os.listdir(path)\n",
        "    print(f\"Contents of {path}:\")\n",
        "    for item in contents:\n",
        "        print(item)\n",
        "\n",
        "files = os.listdir('/content/drive/My Drive/Sensor_dataset')\n",
        "files = os.listdir('/content/drive/My Drive/Sensor_dataset/SP_Sensor')\n",
        "files = os.listdir('/content/drive/My Drive/Sensor_dataset/BP_Sensor')\n",
        "print(files)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#This code utilizes PyOperon and RFECV to determine the best features and their importances.\n",
        "# It then calculates metrics and the best equation using PyOperon for each fold. Finally, it computes the averages of values for SR-pyoperon.\n",
        "\n",
        "!pip install pandas\n",
        "!pip install pyoperon\n",
        "!pip install scikit-learn\n",
        "\n",
        "import os\n",
        "\n",
        "import re\n",
        "import matplotlib.pyplot as plt\n",
        "import pyoperon\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.feature_selection import RFECV\n",
        "from sklearn.model_selection import KFold\n",
        "from sklearn.metrics import r2_score, mean_squared_error, mean_absolute_error\n",
        "from sklearn.linear_model import LinearRegression\n",
        "from pyoperon.sklearn import SymbolicRegressor\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "\n",
        "\n",
        "\n",
        "def load_datasets():\n",
        "    dataset_paths = [\n",
        "        '/content/drive/My Drive/Sensor_dataset/feat_fold_0.csv',\n",
        "        '/content/drive/My Drive/Sensor_dataset/feat_fold_1.csv',\n",
        "        '/content/drive/My Drive/Sensor_dataset/feat_fold_2.csv',\n",
        "        '/content/drive/My Drive/Sensor_dataset/feat_fold_3.csv',\n",
        "        '/content/drive/My Drive/Sensor_dataset/feat_fold_4.csv'\n",
        "    ]\n",
        "    return [pd.read_csv(path) for path in dataset_paths]\n",
        "\n",
        "def split_datasets(folds):\n",
        "    num_folds = len(folds)\n",
        "    splits = []\n",
        "    for r in range(num_folds):\n",
        "        train_indices = list(range(num_folds))\n",
        "        train_indices.remove(r)\n",
        "        test_indices = [r]\n",
        "        train_dfs = [folds[i] for i in train_indices]\n",
        "        test_dfs = [folds[i] for i in test_indices]\n",
        "        splits.append((pd.concat(train_dfs), pd.concat(test_dfs)))\n",
        "    return splits\n",
        "\n",
        "def extract_variables(equation):\n",
        "    return set(re.findall(r'X\\d+', equation))\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "def perform_analysis(splits):\n",
        "    fold_equations = []\n",
        "    selected_feature_importances = {}\n",
        "    results = []\n",
        "    variable_mappings = {}\n",
        "    for i, (train_data, test_data) in enumerate(splits):\n",
        "        # Ensuring SP and DP are excluded properly from features\n",
        "        X_train = train_data.drop(['SP', 'DP', 'trial', 'patient'], axis=1)\n",
        "        y_train = train_data['DP']\n",
        "        X_test = test_data.drop(['SP', 'DP', 'trial', 'patient'], axis=1)\n",
        "        y_test = test_data['DP']\n",
        "        selector = RFECV(LinearRegression(), step=1, cv=KFold(5), scoring='neg_mean_squared_error').fit(X_train, y_train)\n",
        "        best_features = X_train.columns[selector.support_]\n",
        "        # Symbolic Regressor Configuration (Adjusted to Exclude SP and DP)\n",
        "        default_params = {\n",
        "            'allowed_symbols': 'add,sub,mul,div,constant,variable,pow,exp,log,sin,cos,tan,tanh,sqrt,cbrt,square',\n",
        "            'offspring_generator': 'brood',\n",
        "            'initialization_method': 'btc',\n",
        "            'comparison_factor': 0,\n",
        "            'crossover_internal_probability': 0.9,\n",
        "            'epsilon': 1e-05,\n",
        "            'female_selector': 'tournament',\n",
        "            'objectives': ['r2', 'mse', 'mae', 'length', 'rmse', 'c2'],\n",
        "\n",
        "            'mutation_probability': 0.1,\n",
        "            'reinserter': 'keep-best',\n",
        "            'max_evaluations': int(1e6),\n",
        "            'tournament_size': 3,\n",
        "            'pool_size': 50,\n",
        "            'population_size': 100,\n",
        "            'generations': 500,\n",
        "            'time_limit': 90,\n",
        "            'crossover_probability': 1.0,\n",
        "            'mutation_probability': 0.8,\n",
        "            'max_depth':5\n",
        "\n",
        "        }\n",
        "        model = SymbolicRegressor(**default_params).fit(X_train[best_features], y_train)\n",
        "        predictions = model.predict(X_test[best_features])\n",
        "        m = model.model_\n",
        "        best_equation = model.get_model_string(m)\n",
        "        print(best_equation)\n",
        "        fold_equations.append(best_equation)\n",
        "        if predictions.ndim > 1:\n",
        "            predictions = predictions.ravel()\n",
        "\n",
        "        predictions_df = pd.DataFrame({\n",
        "            'Actual': y_test,\n",
        "            'Predicted': predictions\n",
        "        })\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "        mse = mean_squared_error(y_test, predictions)\n",
        "        mse_naive = mean_squared_error(y_test, [np.mean(y_test)] * len(y_test))\n",
        "        mae = mean_absolute_error(y_test, predictions)\n",
        "        mae_naive = mean_absolute_error(y_test, [np.mean(y_test)] * len(y_test))\n",
        "        score = r2_score(y_test, predictions)\n",
        "        errors = predictions.reshape(y_test.shape) - y_test\n",
        "        ME = errors.mean()\n",
        "        SD = errors.std()\n",
        "\n",
        "\n",
        "        variables_used = extract_variables(best_equation)\n",
        "        print(\"Variables used in the equation:\", variables_used)\n",
        "\n",
        "        variable_mappings[f\"Fold {i}\"] = {f\"X{idx}\": X_train.columns[int(idx[1:])] for idx in extract_variables(best_equation) if int(idx[1:]) < len(X_train.columns)}\n",
        "        results.append({\n",
        "            'fold': i,\n",
        "            'MSE': mse,\n",
        "            'MAE': mae,\n",
        "            'R2_score': score,\n",
        "            'MMSE': mse / mse_naive,\n",
        "            'MASE': mae / mae_naive,\n",
        "            'ME': ME,\n",
        "            'SD': SD,\n",
        "            'best_equation': best_equation,\n",
        "            'best_features': list(best_features)\n",
        "                  })\n",
        "\n",
        "\n",
        "\n",
        "        # Store importances from LinearRegression used in RFECV\n",
        "        selected_importances = dict(zip(best_features, abs(selector.estimator_.coef_)))\n",
        "        fold_equations.append(best_equation)\n",
        "\n",
        "\n",
        "        # Store importances\n",
        "\n",
        "        for feature, importance in selected_importances.items():\n",
        "            if feature in selected_feature_importances:\n",
        "                selected_feature_importances[feature].append(importance)\n",
        "            else:\n",
        "                selected_feature_importances[feature] = [importance]\n",
        "\n",
        "    # Average the importances across folds\n",
        "\n",
        "    averaged_selected_importances = {k: np.mean(v) for k, v in selected_feature_importances.items()}\n",
        "    return results,  averaged_selected_importances,   best_equation, fold_equations, variable_mappings\n",
        "\n",
        "\n",
        "def save_variable_mapping_to_csv(variable_mapping, file_path):\n",
        "    # Ensure the directory exists where the file will be saved\n",
        "    os.makedirs(os.path.dirname(file_path), exist_ok=True)\n",
        "\n",
        "    # Prepare a list to hold all rows before writing to CSV\n",
        "    rows = []\n",
        "    for fold, variables in variable_mapping.items():\n",
        "        for index, name in variables.items():\n",
        "            rows.append({'Fold': fold, 'Variable Index': index, 'Variable Name': name})\n",
        "\n",
        "    # Create a DataFrame and write it to CSV\n",
        "    df = pd.DataFrame(rows)\n",
        "    df.to_csv(file_path, index=False)\n",
        "    print(\"Data saved to CSV successfully.\")\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "def main():\n",
        "    folds = load_datasets()\n",
        "    splits = split_datasets(folds)\n",
        "    results, selected_importances,  best_equation, fold_equations,  variable_mapping  = perform_analysis(splits)\n",
        "    save_variable_mapping_to_csv(variable_mapping, '/content/drive/My Drive/Sensor_dataset/DP_Sensor/variable_mapping_sensor_dp.csv')\n",
        "\n",
        "\n",
        "\n",
        "    results_df = pd.DataFrame(results)\n",
        "    results_df.to_csv('/content/drive/My Drive/Sensor_dataset/DP_Sensor/model_performance_results_Sensor_DP_Selected_sensor.csv', index=False)\n",
        "    numeric_columns = results_df.select_dtypes(exclude='object').columns\n",
        "\n",
        "    average_results = results_df[numeric_columns].mean().to_frame().transpose()\n",
        "    average_results.to_csv('/content/drive/My Drive/Sensor_dataset/DP_Sensor/average_model_performance_results_DP_Sensor_Selected_sensor.csv', index=False)\n",
        "    print(\"Average results:\")\n",
        "    print(average_results)\n",
        "\n",
        "\n",
        "    selected_importances_df = pd.DataFrame(list(selected_importances.items()), columns=['Feature_DP', 'Importance_DP']).sort_values(by='Importance_DP', ascending=False)\n",
        "    print(\"Results and importances saved to CSV.\")\n",
        "    print(results_df)\n",
        "\n",
        "\n",
        "    print(\"Selected Feature Importances_DP:\")\n",
        "    print(selected_importances)\n",
        "\n",
        "    selected_importances_df.to_csv('/content/drive/My Drive/Sensor_dataset/DP_Sensor/selected_feature_importances_DP_sensor.csv', index=False)\n",
        "\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 599
        },
        "id": "TMEyzjsgCdqc",
        "outputId": "2fa027ba-20f4-4711-90e4-6fcb4e90a6b8"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (2.0.3)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas) (2023.4)\n",
            "Requirement already satisfied: tzdata>=2022.1 in /usr/local/lib/python3.10/dist-packages (from pandas) (2024.1)\n",
            "Requirement already satisfied: numpy>=1.21.0 in /usr/local/lib/python3.10/dist-packages (from pandas) (1.25.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas) (1.16.0)\n",
            "Collecting pyoperon\n",
            "  Downloading pyoperon-0.3.4-cp310-cp310-manylinux_2_27_x86_64.whl (891 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m892.0/892.0 kB\u001b[0m \u001b[31m4.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: pyoperon\n",
            "Successfully installed pyoperon-0.3.4\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.10/dist-packages (1.2.2)\n",
            "Requirement already satisfied: numpy>=1.17.3 in /usr/local/lib/python3.10/dist-packages (from scikit-learn) (1.25.2)\n",
            "Requirement already satisfied: scipy>=1.3.2 in /usr/local/lib/python3.10/dist-packages (from scikit-learn) (1.11.4)\n",
            "Requirement already satisfied: joblib>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from scikit-learn) (1.4.0)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn) (3.5.0)\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "[Errno 2] No such file or directory: '/content/drive/My Drive/Sensor_dataset/feat_fold_0.csv'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-1-9d07834338f8>\u001b[0m in \u001b[0;36m<cell line: 200>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    199\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    200\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0m__name__\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"__main__\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 201\u001b[0;31m     \u001b[0mmain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-1-9d07834338f8>\u001b[0m in \u001b[0;36mmain\u001b[0;34m()\u001b[0m\n\u001b[1;32m    169\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    170\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mmain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 171\u001b[0;31m     \u001b[0mfolds\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mload_datasets\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    172\u001b[0m     \u001b[0msplits\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msplit_datasets\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfolds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    173\u001b[0m     \u001b[0mresults\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mselected_importances\u001b[0m\u001b[0;34m,\u001b[0m  \u001b[0mbest_equation\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfold_equations\u001b[0m\u001b[0;34m,\u001b[0m  \u001b[0mvariable_mapping\u001b[0m  \u001b[0;34m=\u001b[0m \u001b[0mperform_analysis\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msplits\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-1-9d07834338f8>\u001b[0m in \u001b[0;36mload_datasets\u001b[0;34m()\u001b[0m\n\u001b[1;32m     27\u001b[0m         \u001b[0;34m'/content/drive/My Drive/Sensor_dataset/feat_fold_4.csv'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m     ]\n\u001b[0;32m---> 29\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mpath\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdataset_paths\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     30\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0msplit_datasets\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfolds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-1-9d07834338f8>\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     27\u001b[0m         \u001b[0;34m'/content/drive/My Drive/Sensor_dataset/feat_fold_4.csv'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m     ]\n\u001b[0;32m---> 29\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mpath\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdataset_paths\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     30\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0msplit_datasets\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfolds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001b[0m\n\u001b[1;32m    910\u001b[0m     \u001b[0mkwds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkwds_defaults\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    911\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 912\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    913\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    914\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    575\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    576\u001b[0m     \u001b[0;31m# Create the parser.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 577\u001b[0;31m     \u001b[0mparser\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    578\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    579\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m   1405\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1406\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhandles\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mIOHandles\u001b[0m \u001b[0;34m|\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1407\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1408\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1409\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[0;34m(self, f, engine)\u001b[0m\n\u001b[1;32m   1659\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0;34m\"b\"\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1660\u001b[0m                     \u001b[0mmode\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;34m\"b\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1661\u001b[0;31m             self.handles = get_handle(\n\u001b[0m\u001b[1;32m   1662\u001b[0m                 \u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1663\u001b[0m                 \u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/io/common.py\u001b[0m in \u001b[0;36mget_handle\u001b[0;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[1;32m    857\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencoding\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;34m\"b\"\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    858\u001b[0m             \u001b[0;31m# Encoding\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 859\u001b[0;31m             handle = open(\n\u001b[0m\u001b[1;32m    860\u001b[0m                 \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    861\u001b[0m                 \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/content/drive/My Drive/Sensor_dataset/feat_fold_0.csv'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#This code is employed to generate new folds by extracting features from the best equations obtained from each fold and the features obtained via RFECV.\n",
        "import pandas as pd\n",
        "# Load the CSV file from a specified path\n",
        "file_path = '/content/drive/My Drive/Sensor_dataset/DP_Sensor/variable_mapping_sensor_dp.csv'\n",
        "data = pd.read_csv(file_path)\n",
        "\n",
        "# Display the first few rows of the dataframe to understand its structure\n",
        "print(data.head())\n",
        "\n",
        "# Select the 'Variable Name' column and drop duplicates to get unique names\n",
        "unique_variable_names = data['Variable Name'].drop_duplicates()\n",
        "\n",
        "# Convert the Series of unique variable names to a Python list\n",
        "unique_variable_names_list = unique_variable_names.tolist()\n",
        "print(\"unique_variable_names_list\",unique_variable_names_list )\n",
        "# Define the dataset paths\n",
        "dataset_paths = [\n",
        "     '/content/drive/My Drive/Sensor_dataset/feat_fold_0.csv',\n",
        "        '/content/drive/My Drive/Sensor_dataset/feat_fold_1.csv',\n",
        "        '/content/drive/My Drive/Sensor_dataset/feat_fold_2.csv',\n",
        "        '/content/drive/My Drive/Sensor_dataset/feat_fold_3.csv',\n",
        "        '/content/drive/My Drive/Sensor_dataset/feat_fold_4.csv'\n",
        "    ]\n",
        "\n",
        "\n",
        "# Extract the feature names from the DataFrame\n",
        "feature_names = unique_variable_names_list+['SP', 'DP', 'trial', 'patient']\n",
        "\n",
        "# Loop through each dataset file\n",
        "for i, path in enumerate(dataset_paths):\n",
        "    # Load the dataset\n",
        "    df = pd.read_csv(path)\n",
        "\n",
        "    # Select the required columns based on feature names\n",
        "    selected_df = df.loc[:, df.columns.isin(feature_names)]\n",
        "\n",
        "    # Save the new DataFrame with selected features to a new CSV file\n",
        "    new_file_path = f'/content/drive/My Drive/Sensor_dataset/DP_Sensor/feat_fold_{i}_selected_DP_sensor.csv'\n",
        "    selected_df.to_csv(new_file_path, index=False)\n",
        "\n",
        "    print(f\"Saved selected features for fold {i} to {new_file_path}\")"
      ],
      "metadata": {
        "id": "mM3XyVvtCpvE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# This code evaluates the performance of traditional machine learning models using a training dataset across new folds.\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
        "from sklearn.ensemble import RandomForestRegressor, AdaBoostRegressor\n",
        "from sklearn.svm import SVR\n",
        "from sklearn.neural_network import MLPRegressor\n",
        "import lightgbm as lgb\n",
        "\n",
        "def load_data(file_paths):\n",
        "    return [pd.read_csv(f) for f in file_paths]\n",
        "\n",
        "def train_evaluate_models(X_train, y_train, X_test, y_test):\n",
        "    models = {\n",
        "        'Naive': None,\n",
        "        'LightGBM': lgb.LGBMRegressor(),\n",
        "        'SVR': SVR(),\n",
        "        'Random Forest': RandomForestRegressor(),\n",
        "        'MLP': MLPRegressor(max_iter=500),\n",
        "        'AdaBoost': AdaBoostRegressor()\n",
        "    }\n",
        "\n",
        "    results = []\n",
        "    for name, model in models.items():\n",
        "        if name == 'Naive':\n",
        "            y_pred_test = np.full(shape=(len(y_test),), fill_value=np.mean(y_train))\n",
        "        else:\n",
        "            model.fit(X_train, y_train)\n",
        "            y_pred_test = model.predict(X_test)\n",
        "\n",
        "        mse = mean_squared_error(y_test, y_pred_test)\n",
        "        mae = mean_absolute_error(y_test, y_pred_test)\n",
        "        score = r2_score(y_test, y_pred_test)\n",
        "        errors = y_pred_test - y_test\n",
        "        ME = errors.mean()\n",
        "        SD = errors.std()\n",
        "\n",
        "        mse_naive = mean_squared_error(y_test, [np.mean(y_test)] * len(y_test))\n",
        "\n",
        "        mae_naive = mean_absolute_error(y_test, [np.mean(y_test)] * len(y_test))\n",
        "\n",
        "        results.append({\n",
        "            'Model': name,\n",
        "            'MSE': mse,\n",
        "            'MAE': mae,\n",
        "            'R2_Score': score,\n",
        "            #'MSE_Naive': mse_naive,\n",
        "            #'MAE_Naive': mae_naive ,\n",
        "            'MMSE': mse / mse_naive,\n",
        "            'MASE': mae / mae_naive,\n",
        "            'ME': ME,\n",
        "            'SD': SD\n",
        "        })\n",
        "    return results\n",
        "\n",
        "def main():\n",
        "\n",
        "    selected_files = [\n",
        "        '/content/drive/My Drive/Sensor_dataset/SP_Sensor/feat_fold_0_selected_SP_sensor.csv',\n",
        "        '/content/drive/My Drive/Sensor_dataset/SP_Sensor/feat_fold_1_selected_SP_sensor.csv',\n",
        "        '/content/drive/My Drive/Sensor_dataset/SP_Sensor/feat_fold_2_selected_SP_sensor.csv',\n",
        "        '/content/drive/My Drive/Sensor_dataset/SP_Sensor/feat_fold_3_selected_SP_sensor.csv',\n",
        "        '/content/drive/My Drive/Sensor_dataset/SP_Sensor/feat_fold_4_selected_SP_sensor.csv'\n",
        "    ]\n",
        "\n",
        "\n",
        "    selected_data = load_data(selected_files)\n",
        "\n",
        "\n",
        "    all_selected_results = []\n",
        "\n",
        "    for i in range(len(  selected_data)):\n",
        "\n",
        "        # Selected data processing\n",
        "        train_data, test_data = train_test_split(selected_data[i], test_size=0.2, random_state=42)\n",
        "        X_train_selected = train_data.drop(['SP', 'DP', 'trial', 'patient'], axis=1)\n",
        "        y_train_selected = train_data['SP']\n",
        "        X_test_selected = test_data.drop(['SP', 'DP', 'trial', 'patient'], axis=1)\n",
        "        y_test_selected = test_data['SP']\n",
        "        selected_results = train_evaluate_models(X_train_selected, y_train_selected, X_test_selected, y_test_selected)\n",
        "        all_selected_results.extend(selected_results)\n",
        "\n",
        "    pd.DataFrame(all_selected_results).to_csv('/content/drive/My Drive/Sensor_dataset/SP_Sensor/results_selected_SP_sensor.csv', index=False)\n",
        "    print(\"Results for original and selected datasets have been saved.\")\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()\n"
      ],
      "metadata": {
        "id": "s9sRQQqEEgV6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# This code computes the means of the metrics acquired from traditional models.\n",
        "import pandas as pd\n",
        "\n",
        "def load_results(file_path):\n",
        "    # Load results from a CSV file\n",
        "    return pd.read_csv(file_path)\n",
        "\n",
        "def average_results_by_model(results_df):\n",
        "    # Assuming there's a 'Model' column that labels each row with the model's name\n",
        "    # Group by 'Model' and calculate the mean for each group\n",
        "    model_averages = results_df.groupby('Model').mean()\n",
        "    return model_averages\n",
        "\n",
        "def main():\n",
        "    # File paths\n",
        "\n",
        "    selected_file = '/content/drive/My Drive/Sensor_dataset/DP_Sensor/results_selected_DP_sensor_models.csv'\n",
        "    # Load results\n",
        "\n",
        "    selected_results = load_results(selected_file)\n",
        "\n",
        "    # Calculate averages per model\n",
        "\n",
        "    average_selected = average_results_by_model(selected_results)\n",
        "\n",
        "    # Save the average results per model\n",
        "\n",
        "    average_selected.to_csv('/content/drive/My Drive/Sensor_dataset/DP_Sensor/average_results_per_model_selected_DP_sensor_models.csv')\n",
        "\n",
        "    # Display the results\n",
        "\n",
        "    print(\"Average results per model for selected data:\")\n",
        "    print(average_selected)\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 321
        },
        "id": "fts_J48BFt9r",
        "outputId": "7e369a88-32eb-49ff-ed04-7437bd311fed"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "[Errno 2] No such file or directory: '/content/drive/My Drive/Sensor_dataset/DP_Sensor/results_selected_DP_sensor_models.csv'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-2-6f5ac2f665ec>\u001b[0m in \u001b[0;36m<cell line: 34>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     33\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0m__name__\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"__main__\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 35\u001b[0;31m     \u001b[0mmain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-2-6f5ac2f665ec>\u001b[0m in \u001b[0;36mmain\u001b[0;34m()\u001b[0m\n\u001b[1;32m     17\u001b[0m     \u001b[0;31m# Load results\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 19\u001b[0;31m     \u001b[0mselected_results\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mload_results\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mselected_file\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     20\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m     \u001b[0;31m# Calculate averages per model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-2-6f5ac2f665ec>\u001b[0m in \u001b[0;36mload_results\u001b[0;34m(file_path)\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mload_results\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfile_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0;31m# Load results from a CSV file\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfile_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0maverage_results_by_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresults_df\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001b[0m\n\u001b[1;32m    910\u001b[0m     \u001b[0mkwds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkwds_defaults\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    911\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 912\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    913\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    914\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    575\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    576\u001b[0m     \u001b[0;31m# Create the parser.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 577\u001b[0;31m     \u001b[0mparser\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    578\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    579\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m   1405\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1406\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhandles\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mIOHandles\u001b[0m \u001b[0;34m|\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1407\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1408\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1409\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[0;34m(self, f, engine)\u001b[0m\n\u001b[1;32m   1659\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0;34m\"b\"\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1660\u001b[0m                     \u001b[0mmode\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;34m\"b\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1661\u001b[0;31m             self.handles = get_handle(\n\u001b[0m\u001b[1;32m   1662\u001b[0m                 \u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1663\u001b[0m                 \u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/io/common.py\u001b[0m in \u001b[0;36mget_handle\u001b[0;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[1;32m    857\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencoding\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;34m\"b\"\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    858\u001b[0m             \u001b[0;31m# Encoding\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 859\u001b[0;31m             handle = open(\n\u001b[0m\u001b[1;32m    860\u001b[0m                 \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    861\u001b[0m                 \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/content/drive/My Drive/Sensor_dataset/DP_Sensor/results_selected_DP_sensor_models.csv'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# This code is designed to calculate metrics for SR models by combining data obtained from multiple folds, ultimately identifying the best equation.\n",
        "!pip install pandas\n",
        "!pip install pyoperon\n",
        "!pip install scikit-learn\n",
        "\n",
        "import re\n",
        "import pyoperon\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "from sklearn.model_selection import KFold\n",
        "from sklearn.metrics import r2_score, mean_squared_error, mean_absolute_error\n",
        "from sklearn.linear_model import LinearRegression\n",
        "from pyoperon.sklearn import SymbolicRegressor\n",
        "\n",
        "from sklearn.model_selection import train_test_split, cross_val_score\n",
        "from sklearn.metrics import r2_score, make_scorer, mean_squared_error\n",
        "from sklearn.ensemble import RandomForestRegressor\n",
        "from pyoperon.sklearn import SymbolicRegressor\n",
        "from pyoperon import R2, MSE, InfixFormatter, FitLeastSquares, Interpreter\n",
        "\n",
        "\n",
        "\n",
        "def load_datasets():\n",
        "    dataset_paths = [\n",
        "         '/content/drive/My Drive/DP_BCG/feat_fold_0_selected_DP.csv',\n",
        "        '/content/drive/My Drive/DP_BCG/feat_fold_1_selected_DP.csv',\n",
        "        '/content/drive/My Drive/DP_BCG/feat_fold_2_selected_DP.csv',\n",
        "        '/content/drive/My Drive/DP_BCG/feat_fold_3_selected_DP.csv',\n",
        "        '/content/drive/My Drive/DP_BCG/feat_fold_4_selected_DP.csv'\n",
        "\n",
        "    ]\n",
        "    return [pd.read_csv(path) for path in dataset_paths]\n",
        "\n",
        "def extract_variable_indices(equation):\n",
        "    return set(int(num) for num in re.findall(r'X(\\d+)', equation))\n",
        "def replace_variable_names_with_features(equation, features):\n",
        "    # This function will replace 'Xn' in the equation with actual feature names.\n",
        "    def replace_match(match):\n",
        "        index = int(match.group(1))  # Extract the index number from 'Xn'\n",
        "        if index < len(features):\n",
        "            return features[index]  # Replace 'Xn' with the corresponding feature name\n",
        "        else:\n",
        "            return match.group(0)  # No change if index is out of bounds\n",
        "\n",
        "    # Use a regular expression to replace all occurrences of 'Xn' with feature names\n",
        "    return re.sub(r'X(\\d+)', replace_match, equation)\n",
        "\n",
        "def map_indices_to_names(indices, features):\n",
        "    return {index: features[index] for index in indices if index < len(features)}\n",
        "\n",
        "def run_final_model(folds):\n",
        "\n",
        "    train_data = pd.concat(folds, ignore_index=True)\n",
        "    columns_to_exclude = ['SP', 'DP', 'trial', 'patient']\n",
        "    features_to_use = [col for col in train_data.columns if col not in columns_to_exclude]\n",
        "    print( features_to_use)\n",
        "    X_train = train_data.drop(['SP', 'DP', 'trial', 'patient'], axis=1)\n",
        "    y_train = train_data['DP']\n",
        "    model = SymbolicRegressor(\n",
        "            allowed_symbols=\"add,sub,mul,div,constant,variable,pow,exp,log,sin,cos,tan,tanh,sqrt,cbrt,square\",\n",
        "            #brood_size=10,\n",
        "            comparison_factor=0,\n",
        "            crossover_internal_probability=0.9,\n",
        "            crossover_probability=1.0,\n",
        "            initialization_max_depth= 5,\n",
        "            initialization_max_length= 10,\n",
        "            initialization_method= \"btc\",\n",
        "            irregularity_bias= 0.0,\n",
        "            #optimizer_iterations= 5,\n",
        "            #optimizer='lm',\n",
        "            male_selector= \"tournament\",\n",
        "            epsilon=1e-05,\n",
        "            female_selector=\"tournament\",\n",
        "            max_depth= 10,\n",
        "            max_evaluations= 500,\n",
        "            max_length= 50,\n",
        "            max_selection_pressure= 100,\n",
        "           # model_selection_criterion= \"minimum_description_length\",\n",
        "            mutation_probability= 0.25,\n",
        "            n_threads= 32,\n",
        "            offspring_generator= \"basic\",\n",
        "            reinserter= \"keep-best\",\n",
        "            generations=300,\n",
        "            pool_size=100,\n",
        "            population_size=100,\n",
        "            random_state=None,\n",
        "            #reinserter=\"keep-best\",\n",
        "            time_limit=90,\n",
        "            tournament_size=3,\n",
        "            #uncertainty= [sErr]\n",
        "    )\n",
        "\n",
        "    model.fit(X_train, y_train)\n",
        "    m = model.model_\n",
        "    best_equation =  model.get_model_string(m)\n",
        "    print(best_equation)\n",
        "\n",
        "    best_equation_with_features = replace_variable_names_with_features(best_equation, features_to_use)\n",
        "    print(\"Best Equation with Feature Names:\", best_equation_with_features)\n",
        "\n",
        "\n",
        "    predictions = model.predict(X_train)\n",
        "    if predictions.ndim > 1:\n",
        "            predictions = predictions.ravel()\n",
        "\n",
        "    variable_indices = extract_variable_indices(best_equation)\n",
        "    print(\"Extracted Variable Indices:\", variable_indices)\n",
        "    variable_names = map_indices_to_names(variable_indices, features_to_use)\n",
        "    print(\"Variable Names in Best Equation (Dictionary):\", variable_names)\n",
        "\n",
        "    if predictions.ndim > 1:\n",
        "            predictions = predictions.ravel()\n",
        "\n",
        "    # Assuming predictions and y_train are defined from your model output\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "    mse = mean_squared_error(y_train, predictions)\n",
        "    mse_naive = mean_squared_error(y_train, [np.mean(y_train)] * len(y_train))\n",
        "    mae = mean_absolute_error(y_train, predictions)\n",
        "    mae_naive = mean_absolute_error(y_train, [np.mean(y_train)] * len(y_train))\n",
        "    score = r2_score(y_train, predictions)\n",
        "    errors = predictions.reshape(y_train.shape) -y_train\n",
        "    ME = errors.mean()\n",
        "    SD = errors.std()\n",
        "\n",
        "    results={               'MSE': mse,     'MAE': mae,  'R2_score': score,       'MMSE': mse / mse_naive,                                 'MASE':mae / mae_naive,\n",
        "                                  'ME': ME,  'SD': SD,  'best_equation': best_equation,'features_to_use':features_to_use,'best_equation_with_features':best_equation_with_features}\n",
        "    Variable_names_df=  pd.DataFrame([ map_indices_to_names(variable_indices, features_to_use)])\n",
        "    Variable_names_df.to_csv('/content/drive/My Drive/Sensor_dataset/DP_Sensor/bcg_Combine_Variable_names_dP_sensor.csv', index=False)\n",
        "    print(\"Results saved to '/content/drive/My Drive/Sensor_dataset/DP_Sensor/bcg_Combine_Variable_names_dP_sensor.csv'\")\n",
        "\n",
        "    results_df = pd.DataFrame([results])\n",
        "    results_df.to_csv('/content/drive/My Drive/Sensor_dataset/DP_Sensor/bcg_Combine_model_results_dp_sensor.csv', index=False)\n",
        "    print(\"Results saved to '/content/drive/My Drive/Sensor_dataset/DP_Sensor/bcg_Combine_model_results-dp.csv'\")\n",
        "\n",
        "    return  best_equation, results_df\n",
        "def main():\n",
        "    folds = load_datasets()\n",
        "    best_equation, results_df=run_final_model(folds)\n",
        "    print(results_df)\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "i7UsEDqWFxrj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# This code is designed to calculate metrics for SR models for multiple folds, ultimately identifying the best equations.\n",
        "!pip install pandas\n",
        "!pip install pyoperon\n",
        "!pip install scikit-learn\n",
        "\n",
        "import os\n",
        "\n",
        "import re\n",
        "import matplotlib.pyplot as plt\n",
        "import pyoperon\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.feature_selection import RFECV\n",
        "from sklearn.model_selection import KFold\n",
        "from sklearn.metrics import r2_score, mean_squared_error, mean_absolute_error\n",
        "from sklearn.linear_model import LinearRegression\n",
        "from pyoperon.sklearn import SymbolicRegressor\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "\n",
        "\n",
        "\n",
        "def load_datasets():\n",
        "    dataset_paths = [\n",
        "        '/content/drive/My Drive/Sensor_dataset/SP_Sensor/best_feat_fold_0_selected_SP_sensor.csv',\n",
        "        '/content/drive/My Drive/Sensor_dataset/SP_Sensor/best_feat_fold_1_selected_SP_sensor.csv',\n",
        "        '/content/drive/My Drive/Sensor_dataset/SP_Sensor/best_feat_fold_2_selected_SP_sensor.csv',\n",
        "        '/content/drive/My Drive/Sensor_dataset/SP_Sensor/best_feat_fold_3_selected_SP_sensor.csv',\n",
        "        '/content/drive/My Drive/Sensor_dataset/SP_Sensor/best_feat_fold_4_selected_SP_sensor.csv'\n",
        "    ]\n",
        "\n",
        "    return [pd.read_csv(path) for path in dataset_paths]\n",
        "\n",
        "def split_datasets(folds):\n",
        "    num_folds = len(folds)\n",
        "    splits = []\n",
        "    for r in range(num_folds):\n",
        "        train_indices = list(range(num_folds))\n",
        "        train_indices.remove(r)\n",
        "        test_indices = [r]\n",
        "        train_dfs = [folds[i] for i in train_indices]\n",
        "        test_dfs = [folds[i] for i in test_indices]\n",
        "        splits.append((pd.concat(train_dfs), pd.concat(test_dfs)))\n",
        "    return splits\n",
        "\n",
        "def extract_variables(equation):\n",
        "    return set(re.findall(r'X\\d+', equation))\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "def perform_analysis(splits):\n",
        "    fold_equations = []\n",
        "    selected_feature_importances = {}\n",
        "    results = []\n",
        "    variable_mappings = {}\n",
        "    for i, (train_data, test_data) in enumerate(splits):\n",
        "        # Ensuring SP and DP are excluded properly from features\n",
        "        X_train = train_data.drop(['SP', 'DP', 'trial', 'patient'], axis=1)\n",
        "        y_train = train_data['SP']\n",
        "        X_test = test_data.drop(['SP', 'DP', 'trial', 'patient'], axis=1)\n",
        "        y_test = test_data['SP']\n",
        "        #selector = RFECV(LinearRegression(), step=1, cv=KFold(5), scoring='neg_mean_squared_error').fit(X_train, y_train)\n",
        "        #best_features = X_train.columns[selector.support_]\n",
        "        # Symbolic Regressor Configuration (Adjusted to Exclude SP and DP)\n",
        "        default_params = {\n",
        "            'allowed_symbols': 'add,sub,mul,div,constant,variable,pow,exp,log,sin,cos,tan,tanh,sqrt,cbrt,square',\n",
        "            'offspring_generator': 'brood',\n",
        "            'initialization_method': 'btc',\n",
        "            'comparison_factor': 0,\n",
        "            'crossover_internal_probability': 0.9,\n",
        "            'epsilon': 1e-05,\n",
        "            'female_selector': 'tournament',\n",
        "            'objectives': ['r2', 'mse', 'mae', 'length', 'rmse', 'c2'],\n",
        "\n",
        "            'mutation_probability': 0.1,\n",
        "            'reinserter': 'keep-best',\n",
        "            'max_evaluations': int(1e6),\n",
        "            'tournament_size': 3,\n",
        "            'pool_size': 50,\n",
        "            'population_size': 100,\n",
        "            'generations': 500,\n",
        "            'time_limit': 90,\n",
        "            'crossover_probability': 1.0,\n",
        "            'mutation_probability': 0.8,\n",
        "            'max_depth':5\n",
        "\n",
        "        }\n",
        "        model = SymbolicRegressor(**default_params).fit(X_train, y_train)\n",
        "        predictions = model.predict(X_test)\n",
        "        m = model.model_\n",
        "        best_equation = model.get_model_string(m)\n",
        "        print(best_equation)\n",
        "        fold_equations.append(best_equation)\n",
        "        if predictions.ndim > 1:\n",
        "            predictions = predictions.ravel()\n",
        "\n",
        "        predictions_df = pd.DataFrame({\n",
        "            'Actual': y_test,\n",
        "            'Predicted': predictions\n",
        "        })\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "        mse = mean_squared_error(y_test, predictions)\n",
        "        mse_naive = mean_squared_error(y_test, [np.mean(y_test)] * len(y_test))\n",
        "        mae = mean_absolute_error(y_test, predictions)\n",
        "        mae_naive = mean_absolute_error(y_test, [np.mean(y_test)] * len(y_test))\n",
        "        score = r2_score(y_test, predictions)\n",
        "        errors = predictions.reshape(y_test.shape) - y_test\n",
        "        ME = errors.mean()\n",
        "        SD = errors.std()\n",
        "\n",
        "\n",
        "        variables_used = extract_variables(best_equation)\n",
        "        print(\"Variables used in the equation:\", variables_used)\n",
        "\n",
        "        variable_mappings[f\"Fold {i}\"] = {f\"X{idx}\": X_train.columns[int(idx[1:])] for idx in extract_variables(best_equation) if int(idx[1:]) < len(X_train.columns)}\n",
        "        results.append({\n",
        "            'fold': i,\n",
        "            'MSE': mse,\n",
        "            'MAE': mae,\n",
        "            'R2_score': score,\n",
        "            'MMSE': mse / mse_naive,\n",
        "            'MASE': mae / mae_naive,\n",
        "            'ME': ME,\n",
        "            'SD': SD,\n",
        "            'best_equation': best_equation,\n",
        "\n",
        "                  })\n",
        "\n",
        "\n",
        "    return results,     best_equation, fold_equations, variable_mappings\n",
        "\n",
        "\n",
        "def save_variable_mapping_to_csv(variable_mapping, file_path):\n",
        "    # Ensure the directory exists where the file will be saved\n",
        "    os.makedirs(os.path.dirname(file_path), exist_ok=True)\n",
        "\n",
        "    # Prepare a list to hold all rows before writing to CSV\n",
        "    rows = []\n",
        "    for fold, variables in variable_mapping.items():\n",
        "        for index, name in variables.items():\n",
        "            rows.append({'Fold': fold, 'Variable Index': index, 'Variable Name': name})\n",
        "\n",
        "    # Create a DataFrame and write it to CSV\n",
        "    df = pd.DataFrame(rows)\n",
        "    df.to_csv(file_path, index=False)\n",
        "    print(\"Data saved to CSV successfully.\")\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "def main():\n",
        "    folds = load_datasets()\n",
        "    splits = split_datasets(folds)\n",
        "    results,   best_equation, fold_equations,  variable_mapping  = perform_analysis(splits)\n",
        "    save_variable_mapping_to_csv(variable_mapping, '/content/drive/My Drive/Sensor_dataset/SP_Sensor/best2_variable_mapping_sensor_dp.csv')\n",
        "\n",
        "\n",
        "\n",
        "    results_df = pd.DataFrame(results)\n",
        "    results_df.to_csv('/content/drive/My Drive/Sensor_dataset/SP_Sensor/best2_model_performance_results_Sensor_SP_Selected_sensor.csv', index=False)\n",
        "    numeric_columns = results_df.select_dtypes(exclude='object').columns\n",
        "\n",
        "    average_results = results_df[numeric_columns].mean().to_frame().transpose()\n",
        "    average_results.to_csv('/content/drive/My Drive/Sensor_dataset/SP_Sensor/best2_average_model_performance_results_SP_Sensor_Selected_sensor.csv', index=False)\n",
        "    print(\"Average results:\")\n",
        "    print(average_results)\n",
        "\n",
        "\n",
        "    print(\"Results and importances saved to CSV.\")\n",
        "    print(results_df)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()\n"
      ],
      "metadata": {
        "id": "uIdYA05KHZ3d"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "\n",
        "# Data preparation\n",
        "data = pd.DataFrame({\n",
        "    'Model': ['AdaBoost', 'AdaBoost', 'LightGBM', 'LightGBM', 'MLP', 'MLP', 'RF', 'RF', 'SVR', 'SVR', 'SR-fold', 'SR-fold','SR-combine', 'SR-combine'],\n",
        "    'Feature Selection': ['RFECV', 'EFBEQ-SR', 'RFECV', 'EFBEQ-SR', 'RFECV', 'EFBEQ-SR', 'RFECV', 'EFBEQ-SR', 'RFECV', 'EFBEQ-SR', 'RFECV', 'EFBEQ-SR','RFECV', 'EFBEQ-SR'],\n",
        "    'MASE% SBP': [90.36, 93.90, 82.32, 88.42, 89.97, 92.74, 83.25, 88.22, 97.80, 96.88, 93.04, 96.02,91.06,94.12],\n",
        "    'MASE% DBP': [108.39, 106.22, 92.82, 92.82, 97.17, 97.41, 92.61, 92.90, 96.70, 96.70, 94.82, 96.82,96.73,97.04]\n",
        "})\n",
        "\n",
        "# Plotting setup\n",
        "fig, ax = plt.subplots(2, 1, figsize=(14, 12))\n",
        "\n",
        "# SBP Plot\n",
        "sbp_plot = data.pivot(index=\"Model\", columns=\"Feature Selection\", values=\"MASE% SBP\")\n",
        "sbp_bar = sbp_plot.plot(kind='bar', ax=ax[0], colormap='viridis')\n",
        "ax[0].set_title('MASE% for SBP by Feature Selection Method')\n",
        "ax[0].set_ylabel('MASE%')\n",
        "ax[0].legend(title='Feature Selection', loc='upper left', bbox_to_anchor=(1,1))\n",
        "\n",
        "# DBP Plot\n",
        "dbp_plot = data.pivot(index=\"Model\", columns=\"Feature Selection\", values=\"MASE% DBP\")\n",
        "dbp_bar = dbp_plot.plot(kind='bar', ax=ax[1], colormap='viridis')\n",
        "ax[1].set_title('MASE% for DBP by Feature Selection Method')\n",
        "ax[1].set_ylabel('MASE%')\n",
        "ax[1].legend(title='Feature Selection', loc='upper left', bbox_to_anchor=(1,1))\n",
        "\n",
        "# General settings\n",
        "for a in ax:\n",
        "    a.set_xticklabels(a.get_xticklabels(), rotation=45)\n",
        "    a.set_xlabel('Models')\n",
        "\n",
        "plt.tight_layout(rect=[0, 0, 0.85, 1])  # Adjust the right margin to fit the legend\n",
        "\n",
        "# Save the plot to a file\n",
        "plt.savefig('/content/drive/My Drive/Sensor_dataset/SP_Sensor/performance_comparison_mase_sorted.png')\n",
        "\n",
        "# Show the plot\n",
        "plt.show()\n",
        "\n",
        "# Confirm the plot is saved\n",
        "print(\"Plot saved successfully at '/content/drive/My Drive/Sensor_dataset/SP_Sensor/performance_comparison_mase_sorted.png'\")\n"
      ],
      "metadata": {
        "id": "I0o6RXhMIC7q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "\n",
        "# Data preparation\n",
        "data = pd.DataFrame({\n",
        "    'Model': ['AdaBoost', 'LightGBM', 'MLP', 'Naive', 'RF', 'SVR', 'SR'],\n",
        "    'MASE% SBP': [42.48, 38.55, 82.96, 101.11, 38.89, 89.39, 109.81],\n",
        "    'MASE% DBP': [78.94, 64.37, 127.68, 102.25, 60.85, 93.31, 117.89]\n",
        "})\n",
        "\n",
        "# Sort data by MASE% for SBP and DBP\n",
        "data_sorted_sbp = data.sort_values('MASE% SBP').reset_index(drop=True)\n",
        "data_sorted_dbp = data.sort_values('MASE% DBP').reset_index(drop=True)\n",
        "\n",
        "# Plot setup\n",
        "fig, ax = plt.subplots(2, 1, figsize=(10, 10), sharex=True)\n",
        "\n",
        "# Assigning a unique color for each model using a colormap\n",
        "colors_sbp = plt.cm.viridis(np.linspace(0, 1, len(data_sorted_sbp)))\n",
        "colors_dbp = plt.cm.plasma(np.linspace(0, 1, len(data_sorted_dbp)))\n",
        "\n",
        "# SBP plot\n",
        "ax[0].bar(data_sorted_sbp['Model'], data_sorted_sbp['MASE% SBP'], color=colors_sbp)\n",
        "ax[0].set_title('MASE% for SBP')\n",
        "ax[0].set_ylabel('MASE%')\n",
        "ax[0].set_xticks(range(len(data_sorted_sbp['Model'])))\n",
        "ax[0].set_xticklabels(data_sorted_sbp['Model'], rotation=45)\n",
        "\n",
        "# DBP plot\n",
        "ax[1].bar(data_sorted_dbp['Model'], data_sorted_dbp['MASE% DBP'], color=colors_dbp)\n",
        "ax[1].set_title('MASE% for DBP')\n",
        "ax[1].set_ylabel('MASE%')\n",
        "ax[1].set_xticks(range(len(data_sorted_dbp['Model'])))\n",
        "ax[1].set_xticklabels(data_sorted_dbp['Model'], rotation=45)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.savefig('/content/drive/My Drive/Sensor_dataset/SP_Sensor/performance_bcg-comparison_mase_sorted.png')\n",
        "\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "GPgSn1ZgIIox"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "\n",
        "\n",
        "# Sample DataFrame based on Table X structure, adjust according to actual data\n",
        "data = pd.DataFrame({\n",
        "    'Category': ['Histogram Features', 'Frequency Features', 'Amplitude Features', 'Time Features'],\n",
        "    'Number of Elements': [15, 4, 2, 1]  # Example values, replace with actual counts\n",
        "})\n",
        "\n",
        "# Sorting the data by the number of elements\n",
        "data_sorted = data.sort_values('Number of Elements', ascending=True)\n",
        "\n",
        "# Define a color for each category\n",
        "colors = ['red', 'green', 'blue', 'purple']\n",
        "\n",
        "# Creating a bar plot\n",
        "plt.figure(figsize=(8, 4))\n",
        "plt.barh(data_sorted['Category'], data_sorted['Number of Elements'], color=colors)\n",
        "plt.xlabel('Number of Elements')\n",
        "plt.title('Feature Set Distribution by Category For DBP')\n",
        "plt.tight_layout()\n",
        "\n",
        "# Save and show plot\n",
        "plt.savefig('/content/drive/My Drive/Sensor_dataset/SP_Sensor/featur-bp.png')\n",
        "plt.show()\n",
        "\n",
        "# Confirm the plot is saved\n",
        "print(\"Plot saved successfully at '/content/drive/My Drive/Sensor_dataset/SP_Sensor-bp/featur.png'\")\n"
      ],
      "metadata": {
        "id": "IU-HSjuLIP3T"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}